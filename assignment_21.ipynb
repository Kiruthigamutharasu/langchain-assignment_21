{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e59e5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n",
      "\n",
      "--- Page Content Preview ---\n",
      "\n",
      "\n",
      "--- Metadata ---\n",
      "{'source': 'data/sample.txt'}\n"
     ]
    }
   ],
   "source": [
    "#PART 1 – DOCUMENT LOADERS\n",
    "# TASK 1 : TEXT LOADER\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Step 1: Load the .txt file\n",
    "loader = TextLoader(\"data/sample.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: Print required outputs\n",
    "print(\"Number of documents loaded:\", len(docs))\n",
    "\n",
    "print(\"\\n--- Page Content Preview ---\")\n",
    "print(docs[0].page_content[:300])\n",
    "\n",
    "print(\"\\n--- Metadata ---\")\n",
    "print(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3e80aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV documents: 10\n",
      "\n",
      "--- Sample CSV Document ---\n",
      "id: 1\n",
      "title: LangChain Basics\n",
      "category: AI\n",
      "description: LangChain provides document loaders and text splitters for building LLM applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TASK 2 : CSV LOADER\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Step 1: Load CSV file\n",
    "loader = CSVLoader(\"data/sample.csv\")\n",
    "\n",
    "# Step 2: Convert rows into documents\n",
    "csv_docs = loader.load()\n",
    "\n",
    "# Step 3: Print sample document\n",
    "print(\"Total CSV documents:\", len(csv_docs))\n",
    "\n",
    "print(\"\\n--- Sample CSV Document ---\")\n",
    "print(csv_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d53ec180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pages: 8\n",
      "\n",
      "--- Sample Page Content ---\n",
      "IntroducƟon to LangChain and LLM LimitaƟons \n",
      "Large Language Models have transformed the way soŌware applicaƟons interact with human \n",
      "knowledge. Instead of wriƟng complex rule-based programs, developers can now rely on models that \n",
      "understand natural language and generate meaningful responses.  \n",
      "However, these models do not automaƟcally know about private documents, company policies, or \n",
      "personal notes.  \n",
      "They are trained on general public data and therefore require addiƟonal mechanisms to access\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  TASK 3 : PDF LOADER\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Step 1: Load PDF\n",
    "loader = PyPDFLoader(\"data/sample.pdf\")\n",
    "pdf_docs = loader.load()\n",
    "\n",
    "# Step 2: Print details\n",
    "print(\"Total Pages:\", len(pdf_docs))\n",
    "\n",
    "print(\"\\n--- Sample Page Content ---\")\n",
    "print(pdf_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85343964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents from directory: 19\n",
      "{'source': 'data\\\\sample.txt'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}\n",
      "{'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2026-02-08T12:09:22+05:30', 'author': 'Kiruthiga Mutharasu', 'moddate': '2026-02-08T12:09:22+05:30', 'title': 'Microsoft Word - sample', 'source': 'data\\\\sample.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}\n",
      "{'source': 'data\\\\sample.csv', 'row': 0}\n",
      "{'source': 'data\\\\sample.csv', 'row': 1}\n",
      "{'source': 'data\\\\sample.csv', 'row': 2}\n",
      "{'source': 'data\\\\sample.csv', 'row': 3}\n",
      "{'source': 'data\\\\sample.csv', 'row': 4}\n",
      "{'source': 'data\\\\sample.csv', 'row': 5}\n",
      "{'source': 'data\\\\sample.csv', 'row': 6}\n",
      "{'source': 'data\\\\sample.csv', 'row': 7}\n",
      "{'source': 'data\\\\sample.csv', 'row': 8}\n",
      "{'source': 'data\\\\sample.csv', 'row': 9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 1 - TASK 4 : DIRECTORY LOADER (FIXED)\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "\n",
    "loaders = []\n",
    "\n",
    "# Load TXT files\n",
    "txt_loader = DirectoryLoader(\n",
    "    \"data/\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "loaders.extend(txt_loader.load())\n",
    "\n",
    "\n",
    "# Load PDF files\n",
    "pdf_loader = DirectoryLoader(\n",
    "    \"data/\",\n",
    "    glob=\"*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "loaders.extend(pdf_loader.load())\n",
    "\n",
    "\n",
    "# Load CSV files\n",
    "csv_loader = DirectoryLoader(\n",
    "    \"data/\",\n",
    "    glob=\"*.csv\",\n",
    "    loader_cls=CSVLoader\n",
    ")\n",
    "loaders.extend(csv_loader.load())\n",
    "\n",
    "\n",
    "all_docs = loaders\n",
    "\n",
    "print(\"Total documents from directory:\", len(all_docs))\n",
    "\n",
    "for d in all_docs:\n",
    "    print(d.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abdab7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "Create account\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate Create account Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 1 - TASK 5 : WEB LOADER\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Step 1: Load webpage\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/LangChain\")\n",
    "web_docs = loader.load()\n",
    "\n",
    "# Step 2: Print first 500 characters\n",
    "print(web_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6ba28",
   "metadata": {},
   "source": [
    "# PART 2 - TASK 6 : WHY TEXT SPLITTING REQUIRED\n",
    "\n",
    "1. Why large docs cannot be passed?\n",
    "- Token limits of LLM\n",
    "- High cost\n",
    "- Irrelevant context\n",
    "- Slow response\n",
    "\n",
    "2. Problems chunking solves:\n",
    "- Fits context window\n",
    "- Improves retrieval\n",
    "- Reduces hallucination\n",
    "- Better search relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "909f615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 18\n",
      "\n",
      "Sample Chunk:\n",
      " IntroducƟon to LangChain and LLM LimitaƟons \n",
      "Large Language Models have transformed the way soŌware applicaƟons interact with human \n",
      "knowledge. Instead of wriƟng complex rule-based programs, developers can now rely on models that \n",
      "understand natural language and generate meaningful responses.  \n",
      "However, these models do not automaƟcally know about private documents, company policies, or \n",
      "personal notes.  \n",
      "They are trained on general public data and therefore require addiƟonal mechanisms to access \n",
      "domain speciﬁc informaƟon. LangChain was created to bridge this gap between raw data sources \n",
      "and intelligent language models. \n",
      "LangChain is an open source framework that provides building blocks for creaƟng GeneraƟve AI \n",
      "applicaƟons. It includes modules for loading documents, spliƫ ng long text, generaƟng embeddings, \n",
      "storing vectors, and construcƟng prompts.  \n",
      "Each module solves a pracƟcal engineering problem faced while integraƟng LLMs into real products. \n",
      "Without such a framework, developers would need to write hundreds of lines of custom code for \n",
      "every project. LangChain standardizes these tasks and allows rapid experimentaƟon. \n",
      "One of the biggest challenges with language models is the context window limitaƟon. Models can \n",
      "process only a ﬁxed number of tokens in a single request. Real documents such as research papers, \n",
      "legal contracts, or annual reports oŌen contain tens of thousands of words. Sending the enƟre \n",
      "document to the model is impossible and even if it were possible, the cost and latency would be \n",
      "extremely high. Therefore, external data must be processed intelligently before it reaches the model. \n",
      " \n",
      "Page 2 – Document Loaders and Data IngesƟon \n",
      "The ﬁrst step in any LLM applicaƟon is data ingesƟon. LangChain provides a variety of document \n",
      "loaders that understand diﬀerent ﬁle formats. TextLoader reads plain text ﬁles, CSVLoader handles \n",
      "spreadsheets, PyPDFLoader extracts content from PDF pages, and WebBaseLoader downloads \n",
      "informaƟon from websites.  \n",
      "Each loader converts the original source into a common Document object that contains page content \n",
      "and metadata such as ﬁle name or page number. This uniﬁed structure allows the rest of the pipeline \n",
      "to remain independent of the data source. \n",
      "Consider a company building an internal knowledge assistant. The informaƟon may exist in mulƟple \n",
      "places: product descripƟons in PDFs, customer records in CSV ﬁles, technical guides in text \n",
      "documents, and help arƟcles on the website. Manually combining all these formats would be \n",
      "extremely Ɵme consuming. With LangChain loaders, the same code can read every source and \n",
      "transform it into a consistent representaƟon. This is the foundaƟon for building scalable AI systems. \n",
      "Metadata plays an important role during retrieval. When a PDF is loaded, the system remembers \n",
      "which page the text came from. When a CSV is loaded, the row number and column names are \n",
      "preserved. Later, when the model generates an answer, the applicaƟon can show references to the \n",
      "original source. This increases user trust and makes debugging easier.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 2 - TASK 7 : LENGTH BASED SPLITTER\n",
    "\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(all_docs)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"\\nSample Chunk:\\n\", chunks[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a3c98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks from recursive: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 2 - TASK 8 : RECURSIVE SPLITTER\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "rec_chunks = recursive.split_documents(all_docs)\n",
    "\n",
    "print(\"Chunks from recursive:\", len(rec_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe050e12",
   "metadata": {},
   "source": [
    "# TASK 9 & 10\n",
    "\n",
    "Semantic chunking means:\n",
    "- Splitting based on meaning\n",
    "- Not fixed length\n",
    "- Uses embeddings similarity\n",
    "- Keeps topics together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8613429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PART 3 - TASK 11 : UNIFIED PIPELINE\n",
    "\n",
    "\n",
    "def load_and_split_documents(path_or_url):\n",
    "\n",
    "    if path_or_url.startswith(\"http\"):\n",
    "        loader = WebBaseLoader(path_or_url)\n",
    "\n",
    "    elif path_or_url.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(path_or_url)\n",
    "\n",
    "    elif path_or_url.endswith(\".csv\"):\n",
    "        loader = CSVLoader(path_or_url)\n",
    "\n",
    "    else:\n",
    "        loader = TextLoader(path_or_url)\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872330b",
   "metadata": {},
   "source": [
    "1. Loader Mapping\n",
    "- txt → TextLoader\n",
    "- csv → CSVLoader\n",
    "- pdf → PyPDFLoader\n",
    "- web → WebBaseLoader\n",
    "\n",
    "2. Best Splitter\n",
    "- Small text → Character\n",
    "- Large PDF → Recursive\n",
    "- Web → Recursive\n",
    "\n",
    "3. Overlap Importance\n",
    "- Keeps context\n",
    "- Prevents cut sentences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
